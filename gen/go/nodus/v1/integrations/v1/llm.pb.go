// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.7
// 	protoc        (unknown)
// source: nodus/v1/integrations/v1/llm.proto

package integrationsv1

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	structpb "google.golang.org/protobuf/types/known/structpb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

type LLMProvider int32

const (
	LLMProvider_LLM_PROVIDER_UNSPECIFIED  LLMProvider = 0
	LLMProvider_LLM_PROVIDER_OPENAI       LLMProvider = 1
	LLMProvider_LLM_PROVIDER_ANTHROPIC    LLMProvider = 2
	LLMProvider_LLM_PROVIDER_COHERE       LLMProvider = 3
	LLMProvider_LLM_PROVIDER_HUGGING_FACE LLMProvider = 4
	LLMProvider_LLM_PROVIDER_GOOGLE       LLMProvider = 5
	LLMProvider_LLM_PROVIDER_MISTRAL      LLMProvider = 6
	LLMProvider_LLM_PROVIDER_LOCAL_OLLAMA LLMProvider = 7
	LLMProvider_LLM_PROVIDER_AZURE_OPENAI LLMProvider = 8
	LLMProvider_LLM_PROVIDER_AWS_BEDROCK  LLMProvider = 9
)

// Enum value maps for LLMProvider.
var (
	LLMProvider_name = map[int32]string{
		0: "LLM_PROVIDER_UNSPECIFIED",
		1: "LLM_PROVIDER_OPENAI",
		2: "LLM_PROVIDER_ANTHROPIC",
		3: "LLM_PROVIDER_COHERE",
		4: "LLM_PROVIDER_HUGGING_FACE",
		5: "LLM_PROVIDER_GOOGLE",
		6: "LLM_PROVIDER_MISTRAL",
		7: "LLM_PROVIDER_LOCAL_OLLAMA",
		8: "LLM_PROVIDER_AZURE_OPENAI",
		9: "LLM_PROVIDER_AWS_BEDROCK",
	}
	LLMProvider_value = map[string]int32{
		"LLM_PROVIDER_UNSPECIFIED":  0,
		"LLM_PROVIDER_OPENAI":       1,
		"LLM_PROVIDER_ANTHROPIC":    2,
		"LLM_PROVIDER_COHERE":       3,
		"LLM_PROVIDER_HUGGING_FACE": 4,
		"LLM_PROVIDER_GOOGLE":       5,
		"LLM_PROVIDER_MISTRAL":      6,
		"LLM_PROVIDER_LOCAL_OLLAMA": 7,
		"LLM_PROVIDER_AZURE_OPENAI": 8,
		"LLM_PROVIDER_AWS_BEDROCK":  9,
	}
)

func (x LLMProvider) Enum() *LLMProvider {
	p := new(LLMProvider)
	*p = x
	return p
}

func (x LLMProvider) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (LLMProvider) Descriptor() protoreflect.EnumDescriptor {
	return file_nodus_v1_integrations_v1_llm_proto_enumTypes[0].Descriptor()
}

func (LLMProvider) Type() protoreflect.EnumType {
	return &file_nodus_v1_integrations_v1_llm_proto_enumTypes[0]
}

func (x LLMProvider) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use LLMProvider.Descriptor instead.
func (LLMProvider) EnumDescriptor() ([]byte, []int) {
	return file_nodus_v1_integrations_v1_llm_proto_rawDescGZIP(), []int{0}
}

type LLMConfiguration struct {
	state            protoimpl.MessageState     `protogen:"open.v1"`
	Provider         LLMProvider                `protobuf:"varint,1,opt,name=provider,proto3,enum=nodus.v1.integrations.v1.LLMProvider" json:"provider,omitempty"`
	ModelName        string                     `protobuf:"bytes,2,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	CredentialId     string                     `protobuf:"bytes,3,opt,name=credential_id,json=credentialId,proto3" json:"credential_id,omitempty"`
	BaseUrl          string                     `protobuf:"bytes,4,opt,name=base_url,json=baseUrl,proto3" json:"base_url,omitempty"`
	Headers          map[string]string          `protobuf:"bytes,6,rep,name=headers,proto3" json:"headers,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	RateLimits       *RateLimitConfig           `protobuf:"bytes,7,opt,name=rate_limits,json=rateLimits,proto3" json:"rate_limits,omitempty"`
	ProviderSpecific map[string]*structpb.Value `protobuf:"bytes,8,rep,name=provider_specific,json=providerSpecific,proto3" json:"provider_specific,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *LLMConfiguration) Reset() {
	*x = LLMConfiguration{}
	mi := &file_nodus_v1_integrations_v1_llm_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LLMConfiguration) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LLMConfiguration) ProtoMessage() {}

func (x *LLMConfiguration) ProtoReflect() protoreflect.Message {
	mi := &file_nodus_v1_integrations_v1_llm_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LLMConfiguration.ProtoReflect.Descriptor instead.
func (*LLMConfiguration) Descriptor() ([]byte, []int) {
	return file_nodus_v1_integrations_v1_llm_proto_rawDescGZIP(), []int{0}
}

func (x *LLMConfiguration) GetProvider() LLMProvider {
	if x != nil {
		return x.Provider
	}
	return LLMProvider_LLM_PROVIDER_UNSPECIFIED
}

func (x *LLMConfiguration) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *LLMConfiguration) GetCredentialId() string {
	if x != nil {
		return x.CredentialId
	}
	return ""
}

func (x *LLMConfiguration) GetBaseUrl() string {
	if x != nil {
		return x.BaseUrl
	}
	return ""
}

func (x *LLMConfiguration) GetHeaders() map[string]string {
	if x != nil {
		return x.Headers
	}
	return nil
}

func (x *LLMConfiguration) GetRateLimits() *RateLimitConfig {
	if x != nil {
		return x.RateLimits
	}
	return nil
}

func (x *LLMConfiguration) GetProviderSpecific() map[string]*structpb.Value {
	if x != nil {
		return x.ProviderSpecific
	}
	return nil
}

type LLMParameters struct {
	state                protoimpl.MessageState `protogen:"open.v1"`
	Temperature          float32                `protobuf:"fixed32,1,opt,name=temperature,proto3" json:"temperature,omitempty"`
	MaxTokens            int32                  `protobuf:"varint,2,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`
	TopP                 float32                `protobuf:"fixed32,3,opt,name=top_p,json=topP,proto3" json:"top_p,omitempty"`
	TopK                 float32                `protobuf:"fixed32,4,opt,name=top_k,json=topK,proto3" json:"top_k,omitempty"`
	FrequencyPenalty     float32                `protobuf:"fixed32,5,opt,name=frequency_penalty,json=frequencyPenalty,proto3" json:"frequency_penalty,omitempty"`
	PresencePenalty      float32                `protobuf:"fixed32,6,opt,name=presence_penalty,json=presencePenalty,proto3" json:"presence_penalty,omitempty"`
	StopSequences        []string               `protobuf:"bytes,7,rep,name=stop_sequences,json=stopSequences,proto3" json:"stop_sequences,omitempty"`
	SystemPromptTemplate string                 `protobuf:"bytes,8,opt,name=system_prompt_template,json=systemPromptTemplate,proto3" json:"system_prompt_template,omitempty"`
	unknownFields        protoimpl.UnknownFields
	sizeCache            protoimpl.SizeCache
}

func (x *LLMParameters) Reset() {
	*x = LLMParameters{}
	mi := &file_nodus_v1_integrations_v1_llm_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LLMParameters) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LLMParameters) ProtoMessage() {}

func (x *LLMParameters) ProtoReflect() protoreflect.Message {
	mi := &file_nodus_v1_integrations_v1_llm_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LLMParameters.ProtoReflect.Descriptor instead.
func (*LLMParameters) Descriptor() ([]byte, []int) {
	return file_nodus_v1_integrations_v1_llm_proto_rawDescGZIP(), []int{1}
}

func (x *LLMParameters) GetTemperature() float32 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *LLMParameters) GetMaxTokens() int32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

func (x *LLMParameters) GetTopP() float32 {
	if x != nil {
		return x.TopP
	}
	return 0
}

func (x *LLMParameters) GetTopK() float32 {
	if x != nil {
		return x.TopK
	}
	return 0
}

func (x *LLMParameters) GetFrequencyPenalty() float32 {
	if x != nil {
		return x.FrequencyPenalty
	}
	return 0
}

func (x *LLMParameters) GetPresencePenalty() float32 {
	if x != nil {
		return x.PresencePenalty
	}
	return 0
}

func (x *LLMParameters) GetStopSequences() []string {
	if x != nil {
		return x.StopSequences
	}
	return nil
}

func (x *LLMParameters) GetSystemPromptTemplate() string {
	if x != nil {
		return x.SystemPromptTemplate
	}
	return ""
}

type RateLimitConfig struct {
	state              protoimpl.MessageState `protogen:"open.v1"`
	RequestsPerMinute  int32                  `protobuf:"varint,1,opt,name=requests_per_minute,json=requestsPerMinute,proto3" json:"requests_per_minute,omitempty"`
	TokensPerMinute    int32                  `protobuf:"varint,2,opt,name=tokens_per_minute,json=tokensPerMinute,proto3" json:"tokens_per_minute,omitempty"`
	ConcurrentRequests int32                  `protobuf:"varint,3,opt,name=concurrent_requests,json=concurrentRequests,proto3" json:"concurrent_requests,omitempty"`
	unknownFields      protoimpl.UnknownFields
	sizeCache          protoimpl.SizeCache
}

func (x *RateLimitConfig) Reset() {
	*x = RateLimitConfig{}
	mi := &file_nodus_v1_integrations_v1_llm_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RateLimitConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RateLimitConfig) ProtoMessage() {}

func (x *RateLimitConfig) ProtoReflect() protoreflect.Message {
	mi := &file_nodus_v1_integrations_v1_llm_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RateLimitConfig.ProtoReflect.Descriptor instead.
func (*RateLimitConfig) Descriptor() ([]byte, []int) {
	return file_nodus_v1_integrations_v1_llm_proto_rawDescGZIP(), []int{2}
}

func (x *RateLimitConfig) GetRequestsPerMinute() int32 {
	if x != nil {
		return x.RequestsPerMinute
	}
	return 0
}

func (x *RateLimitConfig) GetTokensPerMinute() int32 {
	if x != nil {
		return x.TokensPerMinute
	}
	return 0
}

func (x *RateLimitConfig) GetConcurrentRequests() int32 {
	if x != nil {
		return x.ConcurrentRequests
	}
	return 0
}

var File_nodus_v1_integrations_v1_llm_proto protoreflect.FileDescriptor

const file_nodus_v1_integrations_v1_llm_proto_rawDesc = "" +
	"\n" +
	"\"nodus/v1/integrations/v1/llm.proto\x12\x18nodus.v1.integrations.v1\x1a\x1cgoogle/protobuf/struct.proto\"\xdb\x04\n" +
	"\x10LLMConfiguration\x12A\n" +
	"\bprovider\x18\x01 \x01(\x0e2%.nodus.v1.integrations.v1.LLMProviderR\bprovider\x12\x1d\n" +
	"\n" +
	"model_name\x18\x02 \x01(\tR\tmodelName\x12#\n" +
	"\rcredential_id\x18\x03 \x01(\tR\fcredentialId\x12\x19\n" +
	"\bbase_url\x18\x04 \x01(\tR\abaseUrl\x12Q\n" +
	"\aheaders\x18\x06 \x03(\v27.nodus.v1.integrations.v1.LLMConfiguration.HeadersEntryR\aheaders\x12J\n" +
	"\vrate_limits\x18\a \x01(\v2).nodus.v1.integrations.v1.RateLimitConfigR\n" +
	"rateLimits\x12m\n" +
	"\x11provider_specific\x18\b \x03(\v2@.nodus.v1.integrations.v1.LLMConfiguration.ProviderSpecificEntryR\x10providerSpecific\x1a:\n" +
	"\fHeadersEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\x1a[\n" +
	"\x15ProviderSpecificEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12,\n" +
	"\x05value\x18\x02 \x01(\v2\x16.google.protobuf.ValueR\x05value:\x028\x01\"\xaf\x02\n" +
	"\rLLMParameters\x12 \n" +
	"\vtemperature\x18\x01 \x01(\x02R\vtemperature\x12\x1d\n" +
	"\n" +
	"max_tokens\x18\x02 \x01(\x05R\tmaxTokens\x12\x13\n" +
	"\x05top_p\x18\x03 \x01(\x02R\x04topP\x12\x13\n" +
	"\x05top_k\x18\x04 \x01(\x02R\x04topK\x12+\n" +
	"\x11frequency_penalty\x18\x05 \x01(\x02R\x10frequencyPenalty\x12)\n" +
	"\x10presence_penalty\x18\x06 \x01(\x02R\x0fpresencePenalty\x12%\n" +
	"\x0estop_sequences\x18\a \x03(\tR\rstopSequences\x124\n" +
	"\x16system_prompt_template\x18\b \x01(\tR\x14systemPromptTemplate\"\x9e\x01\n" +
	"\x0fRateLimitConfig\x12.\n" +
	"\x13requests_per_minute\x18\x01 \x01(\x05R\x11requestsPerMinute\x12*\n" +
	"\x11tokens_per_minute\x18\x02 \x01(\x05R\x0ftokensPerMinute\x12/\n" +
	"\x13concurrent_requests\x18\x03 \x01(\x05R\x12concurrentRequests*\xa7\x02\n" +
	"\vLLMProvider\x12\x1c\n" +
	"\x18LLM_PROVIDER_UNSPECIFIED\x10\x00\x12\x17\n" +
	"\x13LLM_PROVIDER_OPENAI\x10\x01\x12\x1a\n" +
	"\x16LLM_PROVIDER_ANTHROPIC\x10\x02\x12\x17\n" +
	"\x13LLM_PROVIDER_COHERE\x10\x03\x12\x1d\n" +
	"\x19LLM_PROVIDER_HUGGING_FACE\x10\x04\x12\x17\n" +
	"\x13LLM_PROVIDER_GOOGLE\x10\x05\x12\x18\n" +
	"\x14LLM_PROVIDER_MISTRAL\x10\x06\x12\x1d\n" +
	"\x19LLM_PROVIDER_LOCAL_OLLAMA\x10\a\x12\x1d\n" +
	"\x19LLM_PROVIDER_AZURE_OPENAI\x10\b\x12\x1c\n" +
	"\x18LLM_PROVIDER_AWS_BEDROCK\x10\tB\xff\x01\n" +
	"\x1ccom.nodus.v1.integrations.v1B\bLlmProtoP\x01ZRgithub.com/spounge-ai/spounge-proto/gen/go/nodus/v1/integrations/v1;integrationsv1\xa2\x02\x03NVI\xaa\x02\x18Nodus.V1.Integrations.V1\xca\x02\x18Nodus\\V1\\Integrations\\V1\xe2\x02$Nodus\\V1\\Integrations\\V1\\GPBMetadata\xea\x02\x1bNodus::V1::Integrations::V1b\x06proto3"

var (
	file_nodus_v1_integrations_v1_llm_proto_rawDescOnce sync.Once
	file_nodus_v1_integrations_v1_llm_proto_rawDescData []byte
)

func file_nodus_v1_integrations_v1_llm_proto_rawDescGZIP() []byte {
	file_nodus_v1_integrations_v1_llm_proto_rawDescOnce.Do(func() {
		file_nodus_v1_integrations_v1_llm_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_nodus_v1_integrations_v1_llm_proto_rawDesc), len(file_nodus_v1_integrations_v1_llm_proto_rawDesc)))
	})
	return file_nodus_v1_integrations_v1_llm_proto_rawDescData
}

var file_nodus_v1_integrations_v1_llm_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
var file_nodus_v1_integrations_v1_llm_proto_msgTypes = make([]protoimpl.MessageInfo, 5)
var file_nodus_v1_integrations_v1_llm_proto_goTypes = []any{
	(LLMProvider)(0),         // 0: nodus.v1.integrations.v1.LLMProvider
	(*LLMConfiguration)(nil), // 1: nodus.v1.integrations.v1.LLMConfiguration
	(*LLMParameters)(nil),    // 2: nodus.v1.integrations.v1.LLMParameters
	(*RateLimitConfig)(nil),  // 3: nodus.v1.integrations.v1.RateLimitConfig
	nil,                      // 4: nodus.v1.integrations.v1.LLMConfiguration.HeadersEntry
	nil,                      // 5: nodus.v1.integrations.v1.LLMConfiguration.ProviderSpecificEntry
	(*structpb.Value)(nil),   // 6: google.protobuf.Value
}
var file_nodus_v1_integrations_v1_llm_proto_depIdxs = []int32{
	0, // 0: nodus.v1.integrations.v1.LLMConfiguration.provider:type_name -> nodus.v1.integrations.v1.LLMProvider
	4, // 1: nodus.v1.integrations.v1.LLMConfiguration.headers:type_name -> nodus.v1.integrations.v1.LLMConfiguration.HeadersEntry
	3, // 2: nodus.v1.integrations.v1.LLMConfiguration.rate_limits:type_name -> nodus.v1.integrations.v1.RateLimitConfig
	5, // 3: nodus.v1.integrations.v1.LLMConfiguration.provider_specific:type_name -> nodus.v1.integrations.v1.LLMConfiguration.ProviderSpecificEntry
	6, // 4: nodus.v1.integrations.v1.LLMConfiguration.ProviderSpecificEntry.value:type_name -> google.protobuf.Value
	5, // [5:5] is the sub-list for method output_type
	5, // [5:5] is the sub-list for method input_type
	5, // [5:5] is the sub-list for extension type_name
	5, // [5:5] is the sub-list for extension extendee
	0, // [0:5] is the sub-list for field type_name
}

func init() { file_nodus_v1_integrations_v1_llm_proto_init() }
func file_nodus_v1_integrations_v1_llm_proto_init() {
	if File_nodus_v1_integrations_v1_llm_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_nodus_v1_integrations_v1_llm_proto_rawDesc), len(file_nodus_v1_integrations_v1_llm_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   5,
			NumExtensions: 0,
			NumServices:   0,
		},
		GoTypes:           file_nodus_v1_integrations_v1_llm_proto_goTypes,
		DependencyIndexes: file_nodus_v1_integrations_v1_llm_proto_depIdxs,
		EnumInfos:         file_nodus_v1_integrations_v1_llm_proto_enumTypes,
		MessageInfos:      file_nodus_v1_integrations_v1_llm_proto_msgTypes,
	}.Build()
	File_nodus_v1_integrations_v1_llm_proto = out.File
	file_nodus_v1_integrations_v1_llm_proto_goTypes = nil
	file_nodus_v1_integrations_v1_llm_proto_depIdxs = nil
}
